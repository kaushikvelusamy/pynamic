

# Install and build commands for Polaris at ALCF
-----------------------------------------------

rm -rf pynamic/
git clone https://github.com/LLNL/pynamic.git
cd pynamic/pynamic-pyMPI-2.6a1/



module unload PrgEnv-nvhpc/8.3.3 
module load PrgEnv-cray
module load cudatoolkit-standalone 

rm -rf local
mkdir local






Fixing config_pynamic.c
---------------------------


git clean -fxd
export CC="cc"

In file: config_pynamic.c

#Add CC and provide include path
line 31: command = 'env CC="cc" ./configure --prefix=/lus/grand/projects/datascience/kaushikv/copper-test/pynamic/pynamic-pyMPI-2.6a1/local --with-python=/usr/bin/python2 --with-includes=-I/opt/cray/pe/mpich/8.1.16/ofi/cray/10.0/include/ --with-prompt-nl --with-isatty --with-python=%s --with-libs="' % (sys.executable)
#replace gcc with cc
line 78: command = "cc -g addall.c -o addall"


In file: get-symtab-sizes 

delete the following lines

line19: #which ap >& /dev/null
line20: #hasap=$?
line21: #if test $hasap -eq 0
line22: #then
line23: #   sharedlib=`ldd $1 | gawk '{ print $3}' | xargs ap | grep -i -v "file not found"`
line24: #else
line25: #   sharedlib=`ldd $1 | gawk '{ print $3}' | xargs grep '.so' | grep -i -v "file not found"`
line26: #fi


Add the following lines

sharedlib=`ldd $1  | gawk '{ print $3}' ` 
`echo $sharedlib >> tempfile`
`sed -i 's/not//' tempfile`
sharedlib=`cat tempfile`


In file: so_generator.py   

line 366: #command = 'rm -f libpynamic.a'
line 367: #run_command(command)

Also check for "cc" in this file


In file: Makefile.in

line 549: #rm -f lib*


# To run from config_pynamic.c
-----------------------------------

# module load cudatoolkit-standalone

#python2 config_pynamic.py 20 20 -e -u 20 20 -n 20 --with-cc=cc --with-python=/usr/bin/python2

python2 config_pynamic.py 900 1250 -e -u 350 1250 -n 150 --with-cc=cc --with-python=/usr/bin/python2 -j 8



Output 
-------


./get-symtab-sizes pynamic-pyMPI > sharedlib_section_info_pynamic-pyMPI
tail -10 sharedlib_section_info_pynamic-pyMPI

************************************************
summary of pynamic-pyMPI executable and 1176 shared libraries
Size of aggregate total of shared libraries: 2.1GB
Size of aggregate texts of shared libraries: 1.4GB
Size of aggregate data of shared libraries: 38.2MB
Size of aggregate debug sections of shared libraries: 1.1GB
Size of aggregate symbol tables of shared libraries: 56.8MB
Size of aggregate string table size of shared libraries: 330.4MB
************************************************
./get-symtab-sizes pynamic-sdb-pyMPI > sharedlib_section_info_pynamic-sdb-pyMPI
tail -10 sharedlib_section_info_pynamic-sdb-pyMPI

************************************************
summary of pynamic-sdb-pyMPI executable and 1206 shared libraries
Size of aggregate total of shared libraries: 3.7GB
Size of aggregate texts of shared libraries: 2.0GB
Size of aggregate data of shared libraries: 41.1MB
Size of aggregate debug sections of shared libraries: 2.0GB
Size of aggregate symbol tables of shared libraries: 85.6MB
Size of aggregate string table size of shared libraries: 524.6MB
************************************************







Launch local execution
----------------------



(base) kaushikvelusamy@polaris-login-02:/lus/grand/projects/datascience/kaushikv/copper-test/pyna-big/pynamic/pynamic-pyMPI-2.6a1> python2 pynamic_driver.py 
Pynamic: Version 1.3.3
Pynamic: run on 01/01/23 17:36:57 with 1 MPI tasks

Pynamic: driver beginning... now importing modules
Pynamic: driver finished importing all modules... visiting all module functions
Pynamic: module import time = 40.3904719353 secs
Pynamic: module visit time = 1.54450702667 secs
Pynamic: module test passed!











# To run directly from so_generator.py 
----------------------------------------

commented 2 lines in so_generator.py    : line 366:#command = 'rm -f libpynamic.a' and line 367:#run_command(command)
commented 1 line in Makefile.in : line 549: #rm -f lib*


export CC="gcc"
python2 so_generator.py 20 20 
export CC="cc"
env CC='cc' ./configure --prefix=/lus/grand/projects/datascience/kaushikv/copper-test/pynamic/pynamic-pyMPI-2.6a1/local --with-python=/usr/bin/python2 --with-includes=-I/usr/include/
make
make install
python2 pynamic_driver.py # here you may need nvhpc module to avoid shared library loading issue 

#PREFIX=./local make
#PREFIX=./local make install
#PREFIX=./local make install -n



# How my environment looks like at this stage 
------------------------------------------------

(base) kaushikvelusamy@polaris-login-02:> env | grep CC
LMOD_FAMILY_CRAYPE_ACCEL=craype-accel-nvidia80
PE_PRODUCT_LIST=CRAY_PMI:CRAYPE:CRAYPE_X86_ROME:PERFTOOLS:CRAYPAT:CRAY_ACCEL
GCC_X86_64=/opt/cray/pe/gcc/10.3.0/snos
CC=cc
CRAY_ACCEL_VENDOR=nvidia
LMOD_FAMILY_CRAYPE_ACCEL_VERSION=false
CRAY_CCE_CLANGSHARE=/opt/cray/pe/cce/14.0.0/cce-clang/x86_64/share
CRAY_ACCEL_TARGET=nvidia80
GCC_AARCH64=/opt/gcc-cross-aarch64/10.3.0/aarch64
CRAY_CCE_SHARE=/opt/cray/pe/cce/14.0.0/cce/x86_64/share
CC_X86_64=/opt/cray/pe/cce/14.0.0/cce/x86_64
CRAY_PE_CCE_VARIANT=CC=Clang:FTN=Classic
CRAY_CC_VERSION=14.0.0
__LMOD_REF_COUNT_PE_PRODUCT_LIST=CRAY_PMI:1;CRAYPE:1;CRAYPE_X86_ROME:1;PERFTOOLS:1;CRAYPAT:1;CRAY_ACCEL:1

(base) kaushikvelusamy@polaris-login-02:> env | grep CXX
CRAY_CXX_IPA_LIBS_AARCH64=/opt/cray/pe/cce/14.0.0/cce/aarch64/lib/libcray-c++-rts.a
CRAY_CXX_IPA_LIBS=/opt/cray/pe/cce/14.0.0/cce/x86_64/lib/libcray-c++-rts.a
CRAY_CXX_IPA_LIBS_X86_64=/opt/cray/pe/cce/14.0.0/cce/x86_64/lib/libcray-c++-rts.a

(base) kaushikvelusamy@polaris-login-02:> module list
Currently Loaded Modules:
  1) craype-x86-rome          4) perftools-base/22.05.0          7) cce/14.0.0        10) cray-mpich/8.1.16    13) cray-pals/1.1.7
  2) libfabric/1.11.0.4.125   5) craype-accel-nvidia80           8) craype/2.7.15     11) cray-pmi/6.1.2       14) cray-libpals/1.1.7
  3) craype-network-ofi       6) cudatoolkit-standalone/11.7.1   9) cray-dsmml/0.2.2  12) cray-pmi-lib/6.0.17  15) PrgEnv-cray/8.3.3

cudatoolkit-standalone 

(base) kaushikvelusamy@polaris-login-02:~> echo $LD_LIBRARY_PATH
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nvshmeme/lib:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nccl/lib:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/math_libs/lib64:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/lib:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilersextras/qd/lib:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cudaextras/CUPTI/lib64:
/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cuda/lib64:
/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:
/dbhome/db2cat/sqllib/lib64:
/dbhome/db2cat/sqllib/lib64/gskit:
/dbhome/db2cat/sqllib/lib32:
/lus/grand/projects/datascience/kaushikv/copper-test/pynamic/pynamic-pyMPI-2.6a


 

 